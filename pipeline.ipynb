{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "082bd0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Préparation du dataset...\n",
      "Loading of train-clean-100 subset...\n",
      "Loading of train-clean-100 done.\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\Speaker_recognition\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\MATEBOOK D14\\AppData\\Local\\Temp\\ipykernel_15112\\4054316915.py\", line 5, in <module>\n",
      "    train_set, val_set, test_set = prepare_dataset()\n",
      "                                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Speaker_recognition\\utils\\dataloader.py\", line 86, in prepare_dataset\n",
      "    for waveform, sample_rate, _, speaker_id, _, _ in dataset:\n",
      "  File \"d:\\Speaker_recognition\\venv\\Lib\\site-packages\\torchaudio\\datasets\\librispeech.py\", line 170, in __getitem__\n",
      "    waveform = _load_waveform(self._archive, metadata[0], metadata[1])\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Speaker_recognition\\venv\\Lib\\site-packages\\torchaudio\\datasets\\utils.py\", line 51, in _load_waveform\n",
      "    waveform, sample_rate = torchaudio.load(path)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Speaker_recognition\\venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py\", line 205, in load\n",
      "    return backend.load(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Speaker_recognition\\venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile.py\", line 27, in load\n",
      "    return soundfile_backend.load(uri, frame_offset, num_frames, normalize, channels_first, format)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Speaker_recognition\\venv\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py\", line 230, in load\n",
      "    waveform = file_.read(frames, dtype, always_2d=True)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Speaker_recognition\\venv\\Lib\\site-packages\\soundfile.py\", line 938, in read\n",
      "    out = self._create_empty_array(frames, always_2d, dtype)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Speaker_recognition\\venv\\Lib\\site-packages\\soundfile.py\", line 1372, in _create_empty_array\n",
      "    return np.empty(shape, dtype, order='C')\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "numpy._core._exceptions._ArrayMemoryError: Unable to allocate 873. KiB for an array with shape (223600, 1) and data type float32\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Speaker_recognition\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2194, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Speaker_recognition\\venv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1182, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Speaker_recognition\\venv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1053, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Speaker_recognition\\venv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 861, in structured_traceback\n",
      "    formatted_exceptions: list[list[str]] = self.format_exception_as_a_whole(\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Speaker_recognition\\venv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 746, in format_exception_as_a_whole\n",
      "    records = self.get_records(etb, context, tb_offset) if etb else []\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Speaker_recognition\\venv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 848, in get_records\n",
      "    res = list(stack_data.FrameInfo.stack_data(etb, options=options))[tb_offset:]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Speaker_recognition\\venv\\Lib\\site-packages\\stack_data\\core.py\", line 597, in stack_data\n",
      "    yield from collapse_repeated(\n",
      "  File \"d:\\Speaker_recognition\\venv\\Lib\\site-packages\\stack_data\\utils.py\", line 83, in collapse_repeated\n",
      "    yield from map(mapper, original_group)\n",
      "  File \"d:\\Speaker_recognition\\venv\\Lib\\site-packages\\stack_data\\core.py\", line 587, in mapper\n",
      "    return cls(f, options)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Speaker_recognition\\venv\\Lib\\site-packages\\stack_data\\core.py\", line 551, in __init__\n",
      "    self.executing = Source.executing(frame_or_tb)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Speaker_recognition\\venv\\Lib\\site-packages\\executing\\executing.py\", line 264, in executing\n",
      "    source = cls.for_frame(frame)\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Speaker_recognition\\venv\\Lib\\site-packages\\executing\\executing.py\", line 183, in for_frame\n",
      "    return cls.for_filename(frame.f_code.co_filename, frame.f_globals or {}, use_cache)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Speaker_recognition\\venv\\Lib\\site-packages\\executing\\executing.py\", line 212, in for_filename\n",
      "    return cls._for_filename_and_lines(filename, tuple(lines))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Speaker_recognition\\venv\\Lib\\site-packages\\executing\\executing.py\", line 223, in _for_filename_and_lines\n",
      "    result = source_cache[(filename, lines)] = cls(filename, lines)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Speaker_recognition\\venv\\Lib\\site-packages\\executing\\executing.py\", line 163, in __init__\n",
      "    self.tree = ast.parse(self.text, filename=filename)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\ast.py\", line 50, in parse\n",
      "    return compile(source, filename, mode, flags,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "MemoryError\n"
     ]
    }
   ],
   "source": [
    "from utils.dataloader import prepare_dataset\n",
    "from config import config\n",
    "\n",
    "print(\"Préparation du dataset...\")\n",
    "train_set, val_set, test_set = prepare_dataset()\n",
    "\n",
    "print(f\"\\nNombre de locuteurs sélectionnés : {config['num_speakers']}\")\n",
    "print(f\"Durée d'un segment audio : {config['segment_duration']} secondes\")\n",
    "print(f\"Type de features : {config['feature_type']}\")\n",
    "\n",
    "print(f\"\\nTaille du set d'entraînement : {len(train_set)}\")\n",
    "print(f\"Taille du set de validation   : {len(val_set)}\")\n",
    "print(f\"Taille du set de test         : {len(test_set)}\")\n",
    "\n",
    "x, y = train_set[0]\n",
    "print(f\"\\nShape d'un échantillon audio (x) : {x.shape}\")\n",
    "print(f\"Label correspondant (y)         : {y}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
