{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f8773a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchaudio librosa matplotlib scikit-learn\n",
    "#!pip install torchsummary\n",
    "#!pip install --upgrade numpy torchsummary\n",
    "#!pip install torchviz\n",
    "#!pip install torch\n",
    "#!pip install torchaudio\n",
    "#!pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb7b906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7dbd53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"sample_rate\": 16000,\n",
    "    \"n_mfcc\": 40,\n",
    "    \"n_mels\": 64,\n",
    "    \"feature_type\": \"mel\",  # or \"mfcc\"\n",
    "    \"num_speakers\": 10,\n",
    "    \"max_length_per_speaker\": 10.0,\n",
    "    \"batch_size\": 32, #[16, 32, 64],\n",
    "    \"num_epochs\": 10,\n",
    "    \"learning_rate\": 1e-3, #[1e-3, 1e-4, 1e-5],\n",
    "    \"model_type\": \"cnn\",    # [\"cnn\", \"rnn\"]\n",
    "    \"k_folds\": 5,\n",
    "    \"train_vol\": 0.8,\n",
    "    \"val_vol\": 0.2,\n",
    "    \"test_vol\": 0.0,\n",
    "    \"segment_duration\": 3.0,\n",
    "    \"seed\": 42\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85c37b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker Index Map: {'103': 0, '1034': 1, '1040': 2, '1069': 3, '1081': 4, '1088': 5, '1098': 6, '1116': 7, '118': 8, '1183': 9}\n"
     ]
    }
   ],
   "source": [
    "LIBRISPEECH_DIR = \"Data/train-clean-100/LibriSpeech/train-clean-100/\"  \n",
    "\n",
    "# List of speaker folders\n",
    "speaker_dirs = sorted(os.listdir(LIBRISPEECH_DIR))[:config[\"num_speakers\"]]\n",
    "speaker_to_idx = {spk: idx for idx, spk in enumerate(speaker_dirs)}\n",
    "print(\"Speaker Index Map:\", speaker_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0290f5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibriSpeechSpeakerDataset(Dataset):\n",
    "    def __init__(self, root_dir, speaker_to_idx, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.speaker_to_idx = speaker_to_idx\n",
    "        self.target_length = 800  # Fixed number of time steps (adjustable)\n",
    "\n",
    "        for spk in speaker_to_idx.keys():\n",
    "            spk_dir = os.path.join(root_dir, spk)\n",
    "            for chapter in os.listdir(spk_dir):\n",
    "                chapter_dir = os.path.join(spk_dir, chapter)\n",
    "                for file in os.listdir(chapter_dir):\n",
    "                    if file.endswith(\".flac\"):\n",
    "                        self.samples.append({\n",
    "                            \"path\": os.path.join(chapter_dir, file),\n",
    "                            \"label\": speaker_to_idx[spk]\n",
    "                        })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        waveform, sample_rate = torchaudio.load(sample[\"path\"])\n",
    "\n",
    "        # Resample to 16kHz if needed\n",
    "        if sample_rate != config[\"sample_rate\"]:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=config[\"sample_rate\"])\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        # Convert to Mel spectrogram (shape: [1, 64, time])\n",
    "        mel_spec = MelSpectrogram(sample_rate=config[\"sample_rate\"], n_mels=config[\"n_mels\"])(waveform)\n",
    "        mel_db = AmplitudeToDB()(mel_spec).squeeze(0)  # shape: [64, time]\n",
    "\n",
    "        # âœ… Trim or pad to fixed width (target_length = 800)\n",
    "        if mel_db.shape[1] > self.target_length:\n",
    "            mel_db = mel_db[:, :self.target_length]\n",
    "        else:\n",
    "            pad_width = self.target_length - mel_db.shape[1]\n",
    "            mel_db = torch.nn.functional.pad(mel_db, (0, pad_width))\n",
    "            \n",
    "        mel_db = mel_db.unsqueeze(0)  # Add this line BEFORE returning mel_db\n",
    "        return mel_db, sample[\"label\"]\n",
    "\n",
    "\n",
    "        #return mel_db, sample[\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96de617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffd8f61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LibriSpeechSpeakerDataset(LIBRISPEECH_DIR, speaker_to_idx)\n",
    "\n",
    "# Shuffle and split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "#for bs in config[\"batch_size\"]:\n",
    "    #train_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
    "    #val_loader = DataLoader(val_set, batch_size=bs)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=config[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "034b5567",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeakerCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Step 1: Define your conv layers first\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        # Dynamically calculate flattened size after conv layers\n",
    "        n_size = self._get_conv_output((1, 64, 800))\n",
    "        self.fc1 = nn.Linear(n_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        bs = 1\n",
    "        input = torch.rand(bs, *shape)\n",
    "        output_feat = self.conv_layers(input)\n",
    "        n_size = output_feat.view(bs, -1).size(1)\n",
    "        return n_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beac505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#import torchaudio\n",
    "#print(torchaudio.list_audio_backends())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b070d4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\h_ale\\AppData\\Local\\Temp\\ipykernel_30404\\522754334.py:5: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"ffmpeg\")\n"
     ]
    }
   ],
   "source": [
    "#import torchaudio\n",
    "#torchaudio.set_audio_backend(\"sox_io\")  # or \"ffmpeg\" if you used that before\n",
    "\n",
    "#import torchaudio\n",
    "#torchaudio.set_audio_backend(\"ffmpeg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d4748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\h_ale\\AppData\\Local\\Temp\\ipykernel_30404\\1623447431.py:1: UserWarning: torchaudio._backend.get_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  print(torchaudio.get_audio_backend())  # should print 'ffmpeg'\n"
     ]
    }
   ],
   "source": [
    "#print(torchaudio.get_audio_backend())  # should print 'ffmpeg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b506d1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch [1/10] - Loss: 23.9889, Accuracy: 31.43%\n",
      "Epoch [2/10] - Loss: 0.5850, Accuracy: 86.07%\n",
      "Epoch [3/10] - Loss: 0.0629, Accuracy: 98.33%\n",
      "Epoch [4/10] - Loss: 0.0113, Accuracy: 99.88%\n",
      "Epoch [5/10] - Loss: 0.0023, Accuracy: 100.00%\n",
      "Epoch [6/10] - Loss: 0.0006, Accuracy: 100.00%\n",
      "Epoch [7/10] - Loss: 0.0003, Accuracy: 100.00%\n",
      "Epoch [8/10] - Loss: 0.0002, Accuracy: 100.00%\n",
      "Epoch [9/10] - Loss: 0.0001, Accuracy: 100.00%\n",
      "Epoch [10/10] - Loss: 0.0001, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Create model\n",
    "model = SpeakerCNN(num_classes=config[\"num_speakers\"]).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "# Training loop\n",
    "num_epochs = config[\"num_epochs\"]\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for mel, labels in train_loader:\n",
    "        mel = mel.to(device)  # shape: [batch_size, 64, 800]\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(mel)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f}, Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2ff75f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 93.81%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for mel, labels in val_loader:\n",
    "        mel, labels = mel.to(device), labels.to(device)\n",
    "        outputs = model(mel)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6b113dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"speaker_recognition_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d1d68ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 16, 64, 800]             160\n",
      "              ReLU-2          [-1, 16, 64, 800]               0\n",
      "         MaxPool2d-3          [-1, 16, 32, 400]               0\n",
      "            Conv2d-4          [-1, 32, 32, 400]           4,640\n",
      "              ReLU-5          [-1, 32, 32, 400]               0\n",
      "         MaxPool2d-6          [-1, 32, 16, 200]               0\n",
      "            Linear-7                  [-1, 128]      13,107,328\n",
      "            Linear-8                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 13,113,418\n",
      "Trainable params: 13,113,418\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.20\n",
      "Forward/backward pass size (MB): 21.09\n",
      "Params size (MB): 50.02\n",
      "Estimated Total Size (MB): 71.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#summary(model.to(\"cpu\"), input_size=(1, 64, 800), device=\"cpu\")\n",
    "\n",
    "# Show model architecture and parameter sizes (example input: [1, 1, 64, 800])\n",
    "summary(model, input_size=(1, 64, 800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4332849",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Get a sample mel spectrogram from your dataset\n",
    "sample_mel, sample_label = dataset[0]  # First sample\n",
    "sample_mel = sample_mel.squeeze(0)     # Remove channel dim â†’ [64, time]\n",
    "\n",
    "# Plot it\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(sample_mel.numpy(), aspect='auto', origin='lower')\n",
    "plt.title(f\"Mel Spectrogram (Speaker {sample_label})\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Mel Frequency Bins\")\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c875653",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#import matplotlib.pyplot as plt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Get a sample mel spectrogram from your dataset\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m sample_mel, sample_label \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# First sample\u001b[39;00m\n\u001b[0;32m      5\u001b[0m sample_mel \u001b[38;5;241m=\u001b[39m sample_mel\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)     \u001b[38;5;66;03m# Remove channel dim â†’ [64, time]\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Plot it\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Get a sample mel spectrogram from your dataset\n",
    "sample_mel, sample_label = dataset[1]  # First sample\n",
    "sample_mel = sample_mel.squeeze(0)     # Remove channel dim â†’ [64, time]\n",
    "\n",
    "# Plot it\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(sample_mel.numpy(), aspect='auto', origin='lower')\n",
    "plt.title(f\"Mel Spectrogram (Speaker {sample_label})\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Mel Frequency Bins\")\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48fd73fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNModel(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (fc1): Linear(in_features=3136, out_features=256, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#import torch\n",
    "import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "\n",
    "# Define a CNN model similar to the architecture shown in the image\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_channels=1, num_classes=10):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=5, padding=2)  # n1 filters\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)              # n2 filters\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 256)  # adjust based on input size\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Conv1 + ReLU + MaxPool\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Conv2 + ReLU + MaxPool\n",
    "        x = x.view(-1, 64 * 7 * 7)            # Flatten\n",
    "        x = F.relu(self.fc1(x))               # FC1 + ReLU\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)                       # FC2\n",
    "        return x\n",
    "\n",
    "model = CNNModel()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eceab4e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchviz\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_dot\n\u001b[0;32m      6\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)\n\u001b[0;32m      7\u001b[0m y \u001b[38;5;241m=\u001b[39m model(x)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchviz'"
     ]
    }
   ],
   "source": [
    "# Optional: visualize model graph with torchviz\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchviz import make_dot\n",
    "\n",
    "x = torch.randn(1, 1, 28, 28)\n",
    "y = model(x)\n",
    "make_dot(y, params=dict(list(model.named_parameters()))).render(\"cnn_architecture\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1475803d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def draw_cnn_architecture():\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "    # INPUT\n",
    "    ax.text(0.5, 5.5, 'INPUT\\n(28Ã—28Ã—1)', ha='center', va='center', fontsize=12)\n",
    "    ax.add_patch(patches.Rectangle((0, 5), 0.5, 1, edgecolor='black', facecolor='gray'))\n",
    "\n",
    "    # Conv1\n",
    "    ax.text(1.5, 5.5, 'Conv1\\n(5Ã—5, 32)', ha='center', va='center', fontsize=12)\n",
    "    ax.add_patch(patches.Rectangle((1, 4.5), 1, 2, edgecolor='black', facecolor='lightblue'))\n",
    "\n",
    "    # MaxPool1\n",
    "    ax.text(3.2, 5.5, 'MaxPool\\n(2Ã—2)', ha='center', va='center', fontsize=12)\n",
    "    ax.add_patch(patches.Rectangle((2.5, 4.5), 1.2, 2, edgecolor='black', facecolor='skyblue'))\n",
    "\n",
    "    # Conv2\n",
    "    ax.text(5.3, 5.5, 'Conv2\\n(5Ã—5, 64)', ha='center', va='center', fontsize=12)\n",
    "    ax.add_patch(patches.Rectangle((4.8, 4.5), 1, 2, edgecolor='black', facecolor='lightgreen'))\n",
    "\n",
    "    # MaxPool2\n",
    "    ax.text(6.8, 5.5, 'MaxPool\\n(2Ã—2)', ha='center', va='center', fontsize=12)\n",
    "    ax.add_patch(patches.Rectangle((6.3, 4.5), 1, 2, edgecolor='black', facecolor='palegreen'))\n",
    "\n",
    "    # Flatten\n",
    "    ax.text(8.0, 5.5, 'Flatten\\n(7Ã—7Ã—64)', ha='center', va='center', fontsize=12)\n",
    "    ax.add_patch(patches.Rectangle((7.8, 4.5), 0.5, 2, edgecolor='black', facecolor='orange'))\n",
    "\n",
    "    # FC1\n",
    "    ax.text(9.5, 5.5, 'FC1\\n(256)\\nReLU', ha='center', va='center', fontsize=12)\n",
    "    ax.add_patch(patches.Rectangle((9.2, 4.5), 0.6, 2, edgecolor='black', facecolor='gold'))\n",
    "\n",
    "    # Dropout\n",
    "    ax.text(10.6, 5.5, 'Dropout\\n0.5', ha='center', va='center', fontsize=12)\n",
    "    ax.add_patch(patches.Rectangle((10.3, 4.5), 0.6, 2, edgecolor='black', facecolor='wheat'))\n",
    "\n",
    "    # FC2\n",
    "    ax.text(11.7, 5.5, 'FC2\\n(10)', ha='center', va='center', fontsize=12)\n",
    "    ax.add_patch(patches.Rectangle((11.4, 4.5), 0.6, 2, edgecolor='black', facecolor='red'))\n",
    "\n",
    "    # OUTPUT\n",
    "    ax.text(13.0, 5.5, 'OUTPUT\\n(Classes 0â€“9)', ha='center', va='center', fontsize=12)\n",
    "    ax.add_patch(patches.Circle((13, 5.5), 0.3, edgecolor='black', facecolor='red'))\n",
    "\n",
    "    ax.set_xlim(0, 14)\n",
    "    ax.set_ylim(4, 7)\n",
    "    ax.axis('off')\n",
    "    plt.title(\"CNN Architecture for MNIST-like 28x28 Input\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "draw_cnn_architecture()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
