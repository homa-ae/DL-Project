{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48a22e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "\n",
    "# Create data folder if missing\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "\n",
    "# Dowload archive of train-clean-100 and unzip it in data folder\n",
    "# Or load it if previously downloaded\n",
    "dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=\"train-clean-100\", \n",
    "                                          download=not(os.path.isdir(\"data/LibriSpeech\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e21362e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of audio extracts: 251\n",
      "Total duration of dataset (minutes): 6035.41\n",
      "Average duration per extract (minutes): 24.05\n",
      "Total number of speakers: 251\n",
      "Average number of extracts per speaker: 1.00\n",
      "Number of M: 126, and F:125\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import utils.dataset_metadata_parser as dmp\n",
    "\n",
    "# Load speakers metadata and keep only those in train-clean-100\n",
    "speaker_df = dmp.parse_pipe(\"data/LibriSpeech/SPEAKERS.TXT\")\n",
    "filtered_speaker_df = speaker_df[speaker_df[\"SUBSET\"] == \"train-clean-100\"]\n",
    "\n",
    "# Total number of audio extracts\n",
    "total_extracts = len(filtered_speaker_df)\n",
    "\n",
    "# Total duration of dataset (in minutes)\n",
    "total_duration = filtered_speaker_df[\"MINUTES\"].sum()\n",
    "\n",
    "# Average duration per extract\n",
    "average_duration = filtered_speaker_df[\"MINUTES\"].mean()\n",
    "\n",
    "# Total number of unique speakers (by ID or NAME, depending on what defines a speaker)\n",
    "total_speakers = filtered_speaker_df[\"ID\"].nunique()\n",
    "\n",
    "# Average number of extracts per speaker\n",
    "average_extracts_per_speaker = total_extracts / total_speakers\n",
    "\n",
    "# Number of M and F\n",
    "morf_number = filtered_speaker_df['SEX'].value_counts()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total number of audio extracts: {total_extracts}\")\n",
    "print(f\"Total duration of dataset (minutes): {total_duration:.2f}\")\n",
    "print(f\"Average duration per extract (minutes): {average_duration:.2f}\")\n",
    "print(f\"Total number of speakers: {total_speakers}\")\n",
    "print(f\"Average number of extracts per speaker: {average_extracts_per_speaker:.2f}\")\n",
    "print(f\"Number of M: {morf_number['M']}, and F:{morf_number['F']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cc2d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchaudio.transforms import MFCC\n",
    "\n",
    "# Parameters for MFCC extraction\n",
    "sample_rate = 16000\n",
    "n_mfcc = 20\n",
    "melkwargs = {\n",
    "    \"n_fft\": 400,       # frame size of 25ms\n",
    "    \"hop_length\": 160,  # hop size of 10ms\n",
    "    \"n_mels\": 40        # number of Mel filterbanks\n",
    "}\n",
    "\n",
    "# Initialize the MFCC transform\n",
    "mfcc_transform = MFCC(\n",
    "    sample_rate=sample_rate,\n",
    "    n_mfcc=n_mfcc,\n",
    "    melkwargs=melkwargs\n",
    ")\n",
    "\n",
    "class LibriSpeechMFCC(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset that wraps LibriSpeech and applies MFCC transform.\n",
    "    Returns:\n",
    "        mfcc: Tensor of shape (n_mfcc, time_frames)\n",
    "        speaker_id: int\n",
    "    \"\"\"\n",
    "    def __init__(self, root=\"./data\", url=\"train-clean-100\", download=False, transform=None):\n",
    "        self.dataset = torchaudio.datasets.LIBRISPEECH(root, url=url, download=download)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform, sr, _, speaker_id, _, _ = self.dataset[idx]\n",
    "        # Resample if needed\n",
    "        if sr != sample_rate:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, sample_rate)\n",
    "        # Apply MFCC transform: output shape (channel, n_mfcc, time)\n",
    "        mfcc = self.transform(waveform)\n",
    "        # Remove channel dimension for mono audio\n",
    "        mfcc = mfcc.squeeze(0)  # now (n_mfcc, time)\n",
    "        return mfcc, speaker_id\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure data directory exists and dataset is downloaded\n",
    "    os.makedirs(\"./data\", exist_ok=True)\n",
    "    dataset = LibriSpeechMFCC(root=\"./data\",\n",
    "                              url=\"train-clean-100\",\n",
    "                              download=os.path.isdir(\"./data/LibriSpeech\") == False,\n",
    "                              transform=mfcc_transform)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                             batch_size=16,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=2)\n",
    "\n",
    "    # Iterate one batch to check shapes\n",
    "    for batch_mfcc, batch_speaker in dataloader:\n",
    "        print(\"MFCC batch shape:\", batch_mfcc.shape)      # (batch, n_mfcc, time)\n",
    "        print(\"Speaker IDs:\", batch_speaker)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "affe5d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durée : 1.97 sec\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "info = sf.info(\"./data/LibriSpeech/train-clean-100/19/198/19-198-0000.flac\")\n",
    "print(f\"Durée : {info.frames / info.samplerate:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c52e29d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't find appropriate backend to handle uri ./data\\LibriSpeech\\train-clean-100\\103\\1240\\103-1240-0000.flac and format None.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m speakers = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Parcourir les éléments du dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutterance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeaker_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchapter_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutterance_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_audio_files\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspeakers\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeaker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Speaker_recognition\\venv\\Lib\\site-packages\\torchaudio\\datasets\\librispeech.py:170\u001b[39m, in \u001b[36mLIBRISPEECH.__getitem__\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load the n-th sample from the dataset.\u001b[39;00m\n\u001b[32m    149\u001b[39m \n\u001b[32m    150\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m \u001b[33;03m        Utterance ID\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    169\u001b[39m metadata = \u001b[38;5;28mself\u001b[39m.get_metadata(n)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m waveform = \u001b[43m_load_waveform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_archive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (waveform,) + metadata[\u001b[32m1\u001b[39m:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Speaker_recognition\\venv\\Lib\\site-packages\\torchaudio\\datasets\\utils.py:51\u001b[39m, in \u001b[36m_load_waveform\u001b[39m\u001b[34m(root, filename, exp_sample_rate)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_load_waveform\u001b[39m(\n\u001b[32m     46\u001b[39m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m     47\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m     48\u001b[39m     exp_sample_rate: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m     49\u001b[39m ):\n\u001b[32m     50\u001b[39m     path = os.path.join(root, filename)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     waveform, sample_rate = \u001b[43mtorchaudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exp_sample_rate != sample_rate:\n\u001b[32m     53\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msample rate should be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp_sample_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Speaker_recognition\\venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:204\u001b[39m, in \u001b[36mget_load_func.<locals>.load\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m    119\u001b[39m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    120\u001b[39m     frame_offset: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    126\u001b[39m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    127\u001b[39m ) -> Tuple[torch.Tensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[32m    129\u001b[39m \n\u001b[32m    130\u001b[39m \u001b[33;03m    By default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    202\u001b[39m \u001b[33;03m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     backend = \u001b[43mdispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m backend.load(uri, frame_offset, num_frames, normalize, channels_first, \u001b[38;5;28mformat\u001b[39m, buffer_size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Speaker_recognition\\venv\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:116\u001b[39m, in \u001b[36mget_load_func.<locals>.dispatcher\u001b[39m\u001b[34m(uri, format, backend_name)\u001b[39m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m backend.can_decode(uri, \u001b[38;5;28mformat\u001b[39m):\n\u001b[32m    115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m backend\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find appropriate backend to handle uri \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muri\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and format \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mformat\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Couldn't find appropriate backend to handle uri ./data\\LibriSpeech\\train-clean-100\\103\\1240\\103-1240-0000.flac and format None."
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Initialiser les compteurs\n",
    "num_audio_files = 0\n",
    "total_duration_sec = 0.0\n",
    "speakers = set()\n",
    "\n",
    "# Parcourir les éléments du dataset\n",
    "for waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id in dataset:\n",
    "    num_audio_files += 1\n",
    "    speakers.add(speaker_id)\n",
    "    duration_sec = waveform.shape[1] / sample_rate\n",
    "    total_duration_sec += duration_sec\n",
    "\n",
    "# Afficher les statistiques\n",
    "print(f\"Nombre total d'enregistrements audio : {num_audio_files}\")\n",
    "print(f\"Nombre total de speakers : {len(speakers)}\")\n",
    "print(f\"Durée totale des enregistrements : {total_duration_sec / 3600:.2f} heures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "105a4b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 extraits audio (~10h)\n"
     ]
    }
   ],
   "source": [
    "# Filtrer 10 premiers locuteurs et leurs 100 fichiers chacun (~10 h)\n",
    "locuteurs = sorted({speaker for (_, _, _, speaker, _, _) in dataset})[:10]\n",
    "subset = [(wave, sr, _, spk, _, _) for (wave, sr, _, spk, _, _) in dataset if spk in locuteurs][:1000]\n",
    "print(len(subset), \"extraits audio (~10h)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f765cd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 40, 158])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Speaker_recognition\\venv\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torchaudio.transforms as T\n",
    "mfcc_transform = T.MFCC(sample_rate=16000, n_mfcc=40)\n",
    "waveform, sr, _, speaker_id, _, _ = subset[0]\n",
    "mfcc = mfcc_transform(waveform)\n",
    "print(mfcc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0049def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SpeakerNet(nn.Module):\n",
    "    def __init__(self, num_speakers):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 10 * 20, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_speakers)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "defef44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# batch : liste de tuples (waveform [1,T], sr, ..., speaker, ...)\n",
    "def collate_fn(batch):\n",
    "    # On sépare vagues et labels\n",
    "    waves = [item[0].squeeze(0).t() for item in batch]  # chaque wave [T]\n",
    "    speakers = torch.tensor([item[3] for item in batch])\n",
    "    # Padding : on aligne sur la plus grande longueur T_max\n",
    "    padded_waves = pad_sequence(waves, batch_first=True)  # [B, T_max]\n",
    "    # Retour au format [B,1,T_max]\n",
    "    padded_waves = padded_waves.unsqueeze(1)\n",
    "    # Extraction MFCC sur l'ensemble\n",
    "    mfccs = mfcc_transform(padded_waves)  # [B, n_mfcc, T_feat]\n",
    "    # On transpose si nécessaire selon l'architecture\n",
    "    mfccs = mfccs.unsqueeze(1)  # [B,1,n_mfcc,T_feat]\n",
    "    return mfccs, speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a657e3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 6, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m criterion = nn.CrossEntropyLoss()\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m wave, sr, _, speaker, _, _ \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[32m     14\u001b[39m         \u001b[38;5;66;03m# Extraction MFCC\u001b[39;00m\n\u001b[32m     15\u001b[39m         mfcc = mfcc_transform(wave).unsqueeze(\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# [B,1,40,T]\u001b[39;00m\n\u001b[32m     16\u001b[39m         logits = model(mfcc)\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 6, got 2)"
     ]
    }
   ],
   "source": [
    "# Déterminer le nombre de locuteurs dans `dataset`\n",
    "speaker_ids = {speaker for (_, _, _, speaker, _, _) in dataset}\n",
    "num_speakers = len(speaker_ids)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "model = SpeakerNet(num_speakers=num_speakers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    for wave, sr, _, speaker, _, _ in dataloader:\n",
    "        # Extraction MFCC\n",
    "        mfcc = mfcc_transform(wave).unsqueeze(1)  # [B,1,40,T]\n",
    "        logits = model(mfcc)\n",
    "        loss = criterion(logits, speaker)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch} – loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4468021",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [32, 1, 1, 40, 1314]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m – avg loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Lancer l'entraînement\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, dataloader)\u001b[39m\n\u001b[32m     21\u001b[39m total_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mfccs, speakers \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmfccs\u001b[49m\u001b[43m)\u001b[49m             \u001b[38;5;66;03m# mfccs: [B,1,n_mfcc,T]\u001b[39;00m\n\u001b[32m     24\u001b[39m     loss = criterion(logits, speakers)\n\u001b[32m     25\u001b[39m     optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Speaker_recognition\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Speaker_recognition\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mSpeakerNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fc(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Speaker_recognition\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Speaker_recognition\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Speaker_recognition\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Speaker_recognition\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Speaker_recognition\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Speaker_recognition\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Speaker_recognition\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [32, 1, 1, 40, 1314]"
     ]
    }
   ],
   "source": [
    "speaker_ids = {speaker for (_, _, _, speaker, _, _) in dataset}\n",
    "num_speakers = len(speaker_ids)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# On réutilise la collate_fn définie en §2.4 pour le padding et extraction MFCC\n",
    "\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=32,\n",
    "                        shuffle=True,\n",
    "                        collate_fn=collate_fn)\n",
    "\n",
    "model = SpeakerNet(num_speakers=num_speakers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Entraînement avec batch de MFCCs déjà préparés\n",
    "def train(model, dataloader):\n",
    "    model.train()\n",
    "    for epoch in range(10):\n",
    "        total_loss = 0.0\n",
    "        for mfccs, speakers in dataloader:\n",
    "            logits = model(mfccs)             # mfccs: [B,1,n_mfcc,T]\n",
    "            loss = criterion(logits, speakers)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch} | avg loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Lancer l'entraînement\n",
    "train(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a39e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde\n",
    "torch.save(model.state_dict(), \"pretrained_speaker.pth\")\n",
    "\n",
    "# Pour recharger ultérieurement\n",
    "model = SpeakerNet(num_speakers=num_speakers)\n",
    "model.load_state_dict(torch.load(\"pretrained_speaker.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e4c2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import torch\n",
    "\n",
    "# Split 80/20 train/val\n",
    "from torch.utils.data import random_split\n",
    "train_len = int(0.8 * len(dataset))\n",
    "val_len = len(dataset) - train_len\n",
    "train_ds, val_ds = random_split(dataset, [train_len, val_len])\n",
    "val_loader = DataLoader(val_ds, batch_size=32)\n",
    "\n",
    "# Prédictions et vérité\n",
    "y_true, y_pred = [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for wave, sr, _, speaker, _, _ in val_loader:\n",
    "        mfcc = mfcc_transform(wave).unsqueeze(1)\n",
    "        logits = model(mfcc)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        y_true.extend(speaker.tolist())\n",
    "        y_pred.extend(preds.tolist())\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Matrice de confusion :\")\n",
    "print(cm)\n",
    "\n",
    "# Rapport précision / rappel\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(\"Rapport de classification :\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d7c467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Affichage graphique de la matrice de confusion\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', aspect='auto')\n",
    "plt.title('Matrice de confusion')\n",
    "plt.colorbar()\n",
    "\n",
    "# Etiquettes des axes (speaker IDs)\n",
    "labels = sorted(set(y_true))\n",
    "plt.xticks(range(len(labels)), labels, rotation=45)\n",
    "plt.yticks(range(len(labels)), labels)\n",
    "\n",
    "# Annotations\n",
    "thresh = cm.max() / 2\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment='center',\n",
    "             color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "plt.ylabel('Vérité')\n",
    "plt.xlabel('Prédiction')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
